{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3813a6",
   "metadata": {},
   "source": [
    "# vLLM Inference runs at ALCF from remote notebooks\n",
    "\n",
    "This notebook demonstrates how to run vLLM for serving LLM models on Polaris at ALCF using [Globus Compute](https://funcx.readthedocs.io/en/latest/endpoints.html).  In this example, we will authenticate using Globus Auth. Set up a compute endpoint on Polaris. Register a function that serves models using vLLM with Globus (FuncX) and subsequently launch that function in batch mode remotely so that it runs on Polaris and transfers results back to user.\n",
    "\n",
    "This notebook can be run from anywhere, it only requires a local installation of Globus software (described below) and access to a Globus Compute Endpoint setup by the user on Polaris that has access to vLLM (also described below).\n",
    "\n",
    "This demo uses Globus Compute (can also use Globus Flows if needed). Globus Compute is a remote executor for tasks expressed as python functions that are sent to remote machines following a fire-and-forget model.\n",
    "\n",
    "In this notebook we will first describe necessary setup tasks for the local environment and on Polaris; second, we will describe how to create and test a Globus Compute function that can remotely launch a vLLM on Polaris compute nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ac190-cb6b-494d-9e78-354c1830b518",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "1. Allocation on [Polaris](https://accounts.alcf.anl.gov/#/home)\n",
    "2. An account on [Globus](https://www.globus.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d4cc1",
   "metadata": {},
   "source": [
    "## Local Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d36389",
   "metadata": {},
   "source": [
    "This notebook can be run from anywhere.  The only requirement is a local environment, such as a conda environment or python, that has python 3.10 installed along with the Globus packages `globus_compute_sdk` and `globus_cli`.  For e.g.\n",
    "\n",
    "```bash\n",
    "python3.10 -m venv vllm-globus-env\n",
    "source activate vllm-globus-env/bin/activate\n",
    "pip install notebook globus_compute_sdk globus_cli\n",
    "python -m ipykernel install --user --name vllm-env --display-name \"Python3.10-vllm-env\"\n",
    "jupyter notebook\n",
    "```\n",
    "> **__Note:__** <br>\n",
    "> Change the kernel to point to the vllm env in your notebook. <br/>\n",
    "> The vllm environment on Polaris should also contain the same python version 3.10. It is therefore necessary for this environment on your local machine to have a python version close to this version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60401bc9",
   "metadata": {},
   "source": [
    "## Create a Globus Compute Endpoint on Polaris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f27c7",
   "metadata": {},
   "source": [
    "The first step for a user to execute applications on Polaris through the Globus service is to create a Globus compute endpoint on Polaris.  <b> This requires the user to do a one-time setup task to configure the endpoint </b>.\n",
    "\n",
    "In a shell seperate from this notebook, log into Polaris.  Copy the file included with this notebook called [`vllm_template_config.yaml`](../vllm_template_config.yaml) and [`requirements.txt`](../requirements.txt) to the Polaris filesystem (doesn't matter where).  Inside `vllm_template_config.yaml` you should see options setting your `project name`, your queue (default `debug`), and commands that activate a `conda environment` (as done below) on Polaris.\n",
    "\n",
    "In your shell on Polaris, execute the following commands:\n",
    "\n",
    "```bash\n",
    "module use /soft/modulefiles\n",
    "module load conda\n",
    "conda create -p /eagle/<project_name>/env/vllm_env python==3.10 --y\n",
    "conda activate /eagle/<project_name>/env/vllm_env\n",
    "pip install -r requirements.txt\n",
    "globus-compute-endpoint configure --endpoint-config vllm_template_config.yaml vllm_endpoint\n",
    "globus-compute-endpoint start vllm_endpoint\n",
    "globus-compute-endpoint list\n",
    "```\n",
    "This will create an endpoint and display its status.  Its status should be listed as `running`.  There will also be displayed a unique Endpoint ID in the form of a UUID.  Copy that ID and paste it below as a string assigned to `POLARIS_ENDPOINT_FOR_VLLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "decc84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARIS_ENDPOINT_FOR_VLLM = \"1debb802-53d2-4ccc-ad7c-378b101bcd6c\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac1fca",
   "metadata": {},
   "source": [
    "Your endpoint is now active as a daemon process running on the Polaris login node.  It is communicating with the Globus service and waiting for work.  If you ever want to stop the process you can run:\n",
    "```bash\n",
    "globus-compute-endpoint stop vllm_endpoint\n",
    "```\n",
    "Your process may need to be periodically restarted, for example after Polaris comes back from a maintance period.\n",
    "\n",
    "If you ever need to make changes to your endpoint configuration, you can find the settings file in `~/.globus_compute/vllm_endpoint/config.yaml`.  Edit this file and then restart the endpoint with `globus-compute-endpoint restart vllm_endpoint` to make the changes active.\n",
    "\n",
    "This endpoint will submit work to the `debug` queue or any other queue you have access to since this demo is for learning purposes.  In production, LLM users will be able to submit work to the [demand queue](https://docs.alcf.anl.gov/polaris/running-jobs/#queues) which will give immediate access to Polaris compute nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38640cb7",
   "metadata": {},
   "source": [
    "## Create a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d0caf",
   "metadata": {},
   "source": [
    "We first need to create a python function that wraps around the application call.  We will call it `inference_vllm_polaris`. Ensure you change the cache directories to a project folder you have access to within the function. \n",
    "```bash\n",
    "os.environ['HF_DATASETS_CACHE'] = '/eagle/<project>/vllm/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/eagle/<project>/vllm/.cache'\n",
    "```\n",
    "\n",
    "> __**NOTE:**__ For gated models like Llama 13B and 70B, you will have to request access at https://huggingface.co/meta-llama/. Once access is granted you will generate a token here. Pass this token by huggingface-cli login. Alternatively you can simply use the open source models like facebook/opt-125m model which is served by default by vllm. You will have to also set `HF_HOME` in your basrc file. For e.g. `export HF_HOME=/grand/<project>/cache/huggingface`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a08c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Globus Compute function\n",
    "def inference_vllm_polaris(\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0.8,\n",
    "    model_name: str = 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    tokenizer: str = 'hf-internal-testing/llama-tokenizer',\n",
    "    prompt: str = None,\n",
    "    tensor_parallel_size: int = 4\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Function to infer performance and generate outputs based on the parsed arguments on Polaris supercomputer.\n",
    "\n",
    "    Argument\n",
    "    --------\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        temperature (float): Sampling temperature\n",
    "        model_name (str): Name of the model\n",
    "        tokenizer (str): Name of the tokenizer\n",
    "        prompt (str): Prompt to generate\n",
    "        tensor_parallel_size (int): Size of the tensor parallel. No of GPUs used for inference\n",
    "        download_dir (str): Directory to download the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Import packages\n",
    "    import os\n",
    "    os.environ['HF_DATASETS_CACHE'] = '/eagle/datascience/atanikanti/vllm/.cache' #CHANGE THIS\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/eagle/datascience/atanikanti/vllm/.cache' #CHANGE THIS\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "    # ray.shutdown()\n",
    "    ray.init(_temp_dir='/tmp', address=\"auto\", ignore_reinit_error=True)\n",
    "    import time\n",
    "    import json\n",
    "\n",
    "    # Load image (PIL format)\n",
    "    print(f\"max_tokens: {max_tokens}, temperature: {temperature}, model_name: {model_name}, tokenizer: {tokenizer}, prompt: {prompt}, tensor_parallel_size: {tensor_parallel_size}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    params: SamplingParams = SamplingParams(max_tokens=max_tokens, temperature=temperature)\n",
    "    lm: LLM = LLM(\n",
    "        model=model_name, tokenizer=tokenizer, tensor_parallel_size=tensor_parallel_size\n",
    "    )\n",
    "    if not prompt:\n",
    "        prompts = [\n",
    "            \"Hello, my name is\",\n",
    "            \"The president of the United States is\",\n",
    "            \"The capital of USA is\",\n",
    "            \"The future of AI is\",\n",
    "        ]\n",
    "        outputs = lm.generate(prompts, params)\n",
    "    else:\n",
    "        outputs = lm.generate([prompt], params)\n",
    "    generated_outputs = []\n",
    "    total_num_of_tokens = 0\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        num_tokens = len(output.outputs[0].token_ids)\n",
    "        total_num_of_tokens = total_num_of_tokens + num_tokens\n",
    "        generated_outputs.append({\n",
    "            \"num_tokens\": num_tokens,\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text})\n",
    "    latency = time.time() - start_time\n",
    "    tokens_per_second = total_num_of_tokens / latency     \n",
    "    stats = {\n",
    "        \"latency\": f\"{latency:.2f} sec\",\n",
    "        \"total_num_of_tokens\": total_num_of_tokens,\n",
    "        \"tokens_per_second\": f\"{tokens_per_second:.2f} sec\",\n",
    "        \"generated_outputs\": generated_outputs\n",
    "    }\n",
    "    result = json.dumps(stats)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae3a4e",
   "metadata": {},
   "source": [
    "## Authenticate Client and Test Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764af861",
   "metadata": {},
   "source": [
    "We will now instantiate a Globus Compute client to test the function.  Globus will prompt the user for their credentials if running for the first time.  The user should have a Globus account through their ALCF account and should validate with their ALCF credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d63ca0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Globus Compute client\n",
    "# Import packages\n",
    "from globus_compute_sdk import Client, Executor\n",
    "import time\n",
    "gc = Client()\n",
    "polaris_endpoint_id = POLARIS_ENDPOINT_FOR_VLLM\n",
    "gce = Executor(endpoint_id=polaris_endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67262fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = gce.submit(inference_vllm_polaris, temperature=0.8, model_name='meta-llama/Llama-2-70b-chat-hf', prompt='The president of United States is?', tensor_parallel_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9fd693c4-4fcd-47a9-823c-b3ddfcc14699",
   "metadata": {},
   "outputs": [
    {
     "ename": "TaskExecutionFailed",
     "evalue": "\n Traceback (most recent call last):\n   File \"/var/folders/f9/ff27tdm11x91185x3q1v6_lm0000gq/T/ipykernel_35334/1303094765.py\", line 32, in inference_vllm_polaris\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n     return func(*args, **kwargs)\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 1455, in init\n     bootstrap_address = services.canonicalize_bootstrap_address(address, _temp_dir)\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/services.py\", line 528, in canonicalize_bootstrap_address\n     addr = get_ray_address_from_environment(addr, temp_dir)\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/services.py\", line 421, in get_ray_address_from_environment\n     raise ConnectionError(\n ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting `--address` flag or `RAY_ADDRESS` environment variable.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTaskExecutionFailed\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTaskExecutionFailed\u001b[0m: \n Traceback (most recent call last):\n   File \"/var/folders/f9/ff27tdm11x91185x3q1v6_lm0000gq/T/ipykernel_35334/1303094765.py\", line 32, in inference_vllm_polaris\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n     return func(*args, **kwargs)\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 1455, in init\n     bootstrap_address = services.canonicalize_bootstrap_address(address, _temp_dir)\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/services.py\", line 528, in canonicalize_bootstrap_address\n     addr = get_ray_address_from_environment(addr, temp_dir)\n   File \"/eagle/datascience/atanikanti/envs/vllm_jupyter_env/lib/python3.10/site-packages/ray/_private/services.py\", line 421, in get_ray_address_from_environment\n     raise ConnectionError(\n ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting `--address` flag or `RAY_ADDRESS` environment variable.\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b20d4",
   "metadata": {},
   "source": [
    "## Register Function (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcefda2",
   "metadata": {},
   "source": [
    "Now that the function has been tested and works, register the function with the Globus service.  This will allow the user to call the function from within a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "742fd4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7cf58c64-52f1-43ee-951d-b8326087249e\n"
     ]
    }
   ],
   "source": [
    "inference_vllm_func = gc.register_function(inference_vllm_polaris)\n",
    "\n",
    "print(inference_vllm_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c34c26d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"latency\": \"76.17 sec\", \"total_num_of_tokens\": 2568, \"tokens_per_second\": \"33.71 sec\", \"generated_outputs\": [{\"num_tokens\": 552, \"prompt\": \"Hello, my name is\", \"generated_text\": \" Sherry and I\\'m a shopaholic. It\\'s been two weeks since my last shopping spree and I\\'m feeling withdrawal symptoms.\\\\n\\\\nI know, I know, it\\'s not healthy to be addicted to shopping. But let\\'s be real, who doesn\\'t love the rush of finding the perfect outfit or the satisfaction of scoring a great deal? And let\\'s not forget the endless possibilities of online shopping - it\\'s like a never-ending candy store for adults!\\\\n\\\\nBut alas, my bank account is crying out for mercy. I\\'ve had to resort to eating ramen noodles for breakfast, lunch, and dinner just to make ends meet. And don\\'t even get me started on my credit card debt - it\\'s like a never-ending nightmare that I can\\'t seem to wake up from.\\\\n\\\\nDespite all of this, I can\\'t help but feel a twinge of excitement whenever I pass by a store. It\\'s like my heart is beating faster and my feet are moving on their own, drawn to the siren call of the latest trends and must-haves.\\\\n\\\\nAnd don\\'t even get me started on sale seasons. It\\'s like the universe is conspiring against me, tempting me with discounts and deals that I simply cannot resist. I mean, who doesn\\'t love the idea of getting a designer handbag for 50% off? It\\'s like finding a pot of gold at the end of a rainbow!\\\\n\\\\nBut alas, I must resist. I must steel myself against the temptation of the mall and focus on more important things, like paying off my debt and building a stable financial future. It\\'s not easy, but it\\'s necessary.\\\\n\\\\nSo, I\\'ll be strong. I\\'ll stay away from the mall and focus on more frugal pursuits. I\\'ll find joy in the simple things, like cooking at home and reading a good book. And who knows, maybe one day I\\'ll be able to afford that designer handbag without breaking the bank.\\\\n\\\\nUntil then, I\\'ll just have to keep dreaming and saving. And maybe, just maybe, I\\'ll find a way to make my shopping addiction work for me, instead of against me. After all, isn\\'t that what being an adult is all about - finding ways to make our vices work for us, instead of against us?\"}, {\"num_tokens\": 211, \"prompt\": \"The president of the United States is\", \"generated_text\": \" limited to two four-year terms. If a president serves two terms, they cannot run for re-election. This limit is set by the 22nd Amendment to the US Constitution, which was ratified in 1951.\\\\n\\\\nBefore the 22nd Amendment, there was no limit on the number of terms a president could serve. In fact, President Franklin D. Roosevelt was elected to four terms before the amendment was passed. However, after Roosevelt\\'s death, the amendment was introduced and ratified in order to prevent any future president from staying in office for too long.\\\\n\\\\nIt\\'s worth noting that the 22nd Amendment only applies to the president, not to other elected officials such as members of Congress. Additionally, the amendment does not prohibit a president from serving in other government positions after their term is over, such as serving as a cabinet member or a member of Congress.\"}, {\"num_tokens\": 1024, \"prompt\": \"The capital of France is\", \"generated_text\": \" Paris.\\\\n\\\\n7. The currency of Japan is the yen.\\\\n\\\\n8. The capital of Australia is Canberra.\\\\n\\\\n9. The currency of Canada is the Canadian dollar.\\\\n\\\\n10. The capital of China is Beijing.\\\\n\\\\n11. The currency of Germany is the euro.\\\\n\\\\n12. The capital of India is New Delhi.\\\\n\\\\n13. The currency of the United Kingdom is the pound sterling.\\\\n\\\\n14. The capital of Russia is Moscow.\\\\n\\\\n15. The currency of the United States is the US dollar.\\\\n\\\\n16. The capital of Brazil is Bras\\\\u00edlia.\\\\n\\\\n17. The currency of Mexico is the Mexican peso.\\\\n\\\\n18. The capital of South Africa is Pretoria.\\\\n\\\\n19. The currency of South Africa is the South African rand.\\\\n\\\\n20. The capital of Nigeria is Abuja.\\\\n\\\\n21. The currency of Nigeria is the Nigerian naira.\\\\n\\\\n22. The capital of Egypt is Cairo.\\\\n\\\\n23. The currency of Egypt is the Egyptian pound.\\\\n\\\\n24. The capital of South Korea is Seoul.\\\\n\\\\n25. The currency of South Korea is the South Korean won.\\\\n\\\\n26. The capital of Turkey is Ankara.\\\\n\\\\n27. The currency of Turkey is the Turkish lira.\\\\n\\\\n28. The capital of Saudi Arabia is Riyadh.\\\\n\\\\n29. The currency of Saudi Arabia is the Saudi riyal.\\\\n\\\\n30. The capital of Iran is Tehran.\\\\n\\\\n31. The currency of Iran is the Iranian rial.\\\\n\\\\n32. The capital of Indonesia is Jakarta.\\\\n\\\\n33. The currency of Indonesia is the Indonesian rupiah.\\\\n\\\\n34. The capital of Poland is Warsaw.\\\\n\\\\n35. The currency of Poland is the Polish z\\\\u0142oty.\\\\n\\\\n36. The capital of the Philippines is Manila.\\\\n\\\\n37. The currency of the Philippines is the Philippine peso.\\\\n\\\\n38. The capital of Vietnam is Hanoi.\\\\n\\\\n39. The currency of Vietnam is the Vietnamese dong.\\\\n\\\\n40. The capital of Thailand is Bangkok.\\\\n\\\\n41. The currency of Thailand is the Thai baht.\\\\n\\\\n42. The capital of Malaysia is Kuala Lumpur.\\\\n\\\\n43. The currency of Malaysia is the Malaysian ringgit.\\\\n\\\\n44. The capital of Singapore is Singapore.\\\\n\\\\n45. The currency of Singapore is the Singapore dollar.\\\\n\\\\n46. The capital of Italy is Rome.\\\\n\\\\n47. The currency of Italy is the euro.\\\\n\\\\n48. The capital of Switzerland is Bern.\\\\n\\\\n49. The currency of Switzerland is the Swiss franc.\\\\n\\\\n50. The capital of Sweden is Stockholm.\\\\n\\\\n51. The currency of Sweden is the Swedish krona.\\\\n\\\\n52. The capital of Norway is Oslo.\\\\n\\\\n53. The currency of Norway is the Norwegian krone.\\\\n\\\\n54. The capital of Denmark is Copenhagen.\\\\n\\\\n55. The currency of Denmark is the Danish krone.\\\\n\\\\n56. The capital of Finland is Helsinki.\\\\n\\\\n57. The currency of Finland is the Finnish euro.\\\\n\\\\n58. The capital of Greece is Athens.\\\\n\\\\n59. The currency of Greece is the euro.\\\\n\\\\n60. The capital of Portugal is Lisbon.\\\\n\\\\n61. The currency of Portugal is the euro.\\\\n\\\\n62. The capital of Ireland is Dublin.\\\\n\\\\n63. The currency of Ireland is the euro.\\\\n\\\\n64. The capital of New Zealand is Wellington.\\\\n\\\\n65. The currency of New Zealand is the New Zealand dollar.\\\\n\\\\n66. The capital of Austria is Vienna.\\\\n\\\\n67. The currency of Austria is the euro.\\\\n\\\\n68. The capital of Belgium is Brussels.\\\\n\\\\n69. The currency of Belgium is the euro.\\\\n\\\\n70. The capital of Spain is Madrid.\\\\n\\\\n71. The currency of Spain is the euro.\\\\n\\\\n72. The capital of the Netherlands is Amsterdam.\\\\n\\\\n73. The currency of the Netherlands is the euro.\\\\n\\\\n74. The capital of Luxembourg is Luxembourg City.\\\\n\\\\n75. The currency of Luxembourg is the euro.\\\\n\\\\n76. The capital of Czech Republic is Prague.\\\\n\\\\n77. The currency\"}, {\"num_tokens\": 781, \"prompt\": \"The future of AI is\", \"generated_text\": \" likely to be shaped by a combination of technological advancements, societal needs, and ethical considerations. Here are some potential trends and developments in the field of AI:\\\\n\\\\n1. Increased Adoption: AI is likely to become more ubiquitous in various industries such as healthcare, finance, education, and transportation. As the technology improves, more businesses and organizations are likely to adopt AI solutions to streamline processes, improve efficiency, and reduce costs.\\\\n2. Expansion of Application Areas: AI will increasingly be applied to new areas such as climate change, agriculture, and urban planning. For example, AI can be used to monitor and manage natural resources, predict and prevent natural disasters, and optimize urban planning and infrastructure development.\\\\n3. Continued Advances in Deep Learning: Deep learning, a subset of machine learning, is expected to continue to drive advancements in AI. Deep learning algorithms will become more sophisticated and capable of solving complex problems, leading to breakthroughs in areas such as natural language processing, computer vision, and robotics.\\\\n4. Increased Focus on Explainability and Interpretability: As AI becomes more pervasive, there will be a growing need for explainability and interpretability of AI decision-making processes. This will require the development of new techniques and tools to understand and explain AI models and algorithms.\\\\n5. Ethical Considerations: With the increasing use of AI, there will be a growing need for ethical considerations around its development and deployment. This will include ensuring fairness, transparency, and accountability in AI decision-making processes, as well as addressing concerns around privacy, security, and bias.\\\\n6. Human-AI Collaboration: As AI becomes more advanced, it will be increasingly important to develop systems that can collaborate with humans effectively. This will require the development of new interfaces and systems that can facilitate human-AI collaboration in areas such as decision-making, problem-solving, and creativity.\\\\n7. Robotics and Autonomous Systems: Robotics and autonomous systems will continue to advance, with the development of more sophisticated robots and drones that can perform complex tasks in various environments. This will include the development of autonomous vehicles, robots for manufacturing and logistics, and drones for surveillance and monitoring.\\\\n8. Edge AI: With the proliferation of IoT devices, there will be a growing need for AI to be deployed at the edge of the network, closer to the source of the data. This will require the development of new architectures and systems that can enable edge AI, reducing latency and improving real-time processing.\\\\n9. Quantum AI: Quantum computing has the potential to revolutionize AI by enabling the solution of complex problems that are currently unsolvable with classical computers. Quantum AI will become increasingly important as quantum computing technology advances.\\\\n10. Job Displacement and Retraining: As AI becomes more advanced, there is a risk of job displacement in certain sectors. This will require proactive measures to retrain and upskill workers to ensure they are prepared for the jobs of the future.\\\\n\\\\nIn conclusion, the future of AI is likely to be characterized by increased adoption, expanded application areas, and continued advances in deep learning. However, it is also important to address ethical considerations, ensure explainability and interpretability, and develop systems that can collaborate with humans effectively.\"}]}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future = gce.submit_to_registered_function(kwargs={'model_name' : 'meta-llama/Llama-2-70b-chat-hf'}, function_id=inference_vllm_func)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e4c425-6ecf-4298-b9e1-db32d30646f6",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "1. Set and Forget. Allows remotely launching your functions without ever having to log in to Polaris\n",
    "2. Once the job is submitted, you can submit functions sequentially without the queue closing. The job ends after a period of inactivity ~5 mins.\n",
    "3. A service account can potentially allow sharing function (can define scopes for access levels)\n",
    "4. Can use flows for transferring images/files > 10MB\n",
    "5. Easy to move to a non-scheduler-based system\n",
    "   \n",
    "## Disadvantages\n",
    "1. For multi-node inference models vllm needs a \"ray\" cluster to be first set up. Globus does not allow two operations to be submitted in parallel on the same node.\n",
    "2. The payload can be a maximum of 10MB to use globus compute out of the box by passing it as an argument. If >10 we will have to use globus flows.\n",
    "3. vllm does not currently support multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb87e0-c1f7-43e6-ab7f-cf64759177f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32266c8d-303d-42f9-a448-0f2543ce9123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 vllm env",
   "language": "python",
   "name": "vllm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
