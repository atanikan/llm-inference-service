{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3813a6",
   "metadata": {},
   "source": [
    "# llama cpp Inference runs at ALCF from remote notebooks\n",
    "\n",
    "This notebook demonstrates how to run llama cpp for serving LLM models on Polaris at ALCF using [Globus Compute](https://funcx.readthedocs.io/en/latest/endpoints.html).  In this example, we will authenticate using Globus Auth. Set up a compute endpoint on Polaris. Register a function that serves models using vLLM with Globus (FuncX) and subsequently launch that function in batch mode remotely so that it runs on Polaris and transfers results back to user.\n",
    "\n",
    "This notebook can be run from anywhere, it only requires a local installation of Globus software (described below) and access to a Globus Compute Endpoint setup by the user on Polaris that has access to vLLM (also described below).\n",
    "\n",
    "This demo uses Globus Compute (can also use Globus Flows if needed). Globus Compute is a remote executor for tasks expressed as python functions that are sent to remote machines following a fire-and-forget model.\n",
    "\n",
    "In this notebook we will first describe necessary setup tasks for the local environment and on Polaris; second, we will describe how to create and test a Globus Compute function that can remotely launch a vLLM on Polaris compute nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ac190-cb6b-494d-9e78-354c1830b518",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "1. Allocation on [Polaris](https://accounts.alcf.anl.gov/#/home)\n",
    "2. An account on [Globus](https://www.globus.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d4cc1",
   "metadata": {},
   "source": [
    "## Local Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d36389",
   "metadata": {},
   "source": [
    "This notebook can be run from anywhere.  The only requirement is a local environment, such as a conda environment or python, that has python 3.10 installed along with the Globus packages `globus_compute_sdk` and `globus_cli`.  For e.g.\n",
    "\n",
    "```bash\n",
    "conda create -n llama-cpp-env python==3.10.12 -y\n",
    "conda activate llama-cpp-env  \n",
    "conda install jupyter\n",
    "conda install chardet\n",
    "pip install notebook globus_compute_sdk globus_cli\n",
    "python -m ipykernel install --user --name llama-cpp-env --display-name \"Python3.10-llama-cpp-env\"\n",
    "jupyter notebook\n",
    "```\n",
    "> **__Note:__** <br>\n",
    "> Change the kernel to point to the vllm env in your notebook. <br/>\n",
    "> The vllm environment on Polaris should also contain the same python version 3.10. It is therefore necessary for this environment on your local machine to have a python version close to this version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60401bc9",
   "metadata": {},
   "source": [
    "## Create a Globus Compute Endpoint on Polaris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f27c7",
   "metadata": {},
   "source": [
    "The first step for a user to execute applications on Polaris through the Globus service is to create a Globus compute endpoint on Polaris.  <b> This requires the user to do a one-time setup task to configure the endpoint </b>.\n",
    "\n",
    "In a shell seperate from this notebook, log into Polaris.  Copy the file included with this notebook called [`vllm_template_config.yaml`](../vllm_template_config.yaml) and [`requirements.txt`](../requirements.txt) to the Polaris filesystem (doesn't matter where).  Inside `vllm_template_config.yaml` you should see options setting your `project name`, your queue (default `debug`), and commands that activate a `conda environment` (as done below) on Polaris.\n",
    "\n",
    "In your shell on Polaris, execute the following commands:\n",
    "\n",
    "```bash\n",
    "module use /soft/modulefiles; module load conda\n",
    "conda create -p /home/openinference_svc/envs/llama-cpp-cuda-env python==3.10.12 -y\n",
    "conda activate /home/openinference_svc/envs/llama-cpp-cuda-env\n",
    "module load cudatoolkit-standalone/12.2.2\n",
    "conda install cmake -y\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "python3 -m pip install -r requirements.txt\n",
    "pip install globus-compute-endpoint\n",
    "cmake -B build -DLLAMA_CUDA=ON\n",
    "cmake --build build --config Release -t server\n",
    "globus-compute-endpoint configure --endpoint-config llama_cpp_template_config.yaml llama_cpp_endpoint\n",
    "globus-compute-endpoint start llama_cpp_template_config\n",
    "globus-compute-endpoint list\n",
    "```\n",
    "This will create an endpoint and display its status.  Its status should be listed as `running`.  There will also be displayed a unique Endpoint ID in the form of a UUID.  Copy that ID and paste it below as a string assigned to `POLARIS_ENDPOINT_FOR_LLAMACPP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "decc84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARIS_ENDPOINT_FOR_LLAMACPP_Llama38B = \"77ee899c-3b8e-4c13-8ce6-3cbb7ebd80b5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac1fca",
   "metadata": {},
   "source": [
    "Your endpoint is now active as a daemon process running on the Polaris login node.  It is communicating with the Globus service and waiting for work.  If you ever want to stop the process you can run:\n",
    "```bash\n",
    "globus-compute-endpoint stop llama_cpp_endpoint\n",
    "```\n",
    "Your process may need to be periodically restarted, for example after Polaris comes back from a maintance period.\n",
    "\n",
    "If you ever need to make changes to your endpoint configuration, you can find the settings file in `~/.globus_compute/llama_cpp_endpoint/config.yaml`.  Edit this file and then restart the endpoint with `globus-compute-endpoint restart llama_cpp_endpoint` to make the changes active.\n",
    "\n",
    "This endpoint will submit work to the `debug` queue or any other queue you have access to since this demo is for learning purposes.  In production, LLM users will be able to submit work to the [demand queue](https://docs.alcf.anl.gov/polaris/running-jobs/#queues) which will give immediate access to Polaris compute nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38640cb7",
   "metadata": {},
   "source": [
    "## Create a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0483705c-6d22-45a1-a97c-b4260f6d9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llamacpp(prompt):\n",
    "    import openai\n",
    "    \n",
    "    import socket\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    # Determine the hostname\n",
    "    hostname = socket.gethostname()\n",
    "    os.environ['no_proxy'] = hostname\n",
    "    # Construct the base_url\n",
    "    base_url = f\"http://{hostname}:8000/v1\"\n",
    "    \n",
    "    # Initialize the OpenAI client with the base URL and API key\n",
    "    client = openai.OpenAI(base_url=base_url, api_key=\"cxvff_xxxx\")\n",
    "    \n",
    "    # Send a request to the chat completions endpoint\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Meta-Llama-3-8B-Instruct-Q8_0.gguf\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        logprobs=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    response_dict = response.to_dict()  # This converts the response to a dictionary\n",
    "    \n",
    "    \n",
    "    # Convert the response to a JSON-formatted string\n",
    "    \n",
    "    json_response = json.dumps(response_dict, indent=4)\n",
    "    print(json_response)\n",
    "    \n",
    "    # Convert JSON string back to a Python dictionary\n",
    "    response_dict = json.loads(json_response)\n",
    "    \n",
    "    # Access the content of the assistant's message\n",
    "    assistant_content = response_dict['choices'][0]['message']['content']\n",
    "    \n",
    "    print(\"Assistant's Response Content:\")\n",
    "    print(assistant_content)\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae3a4e",
   "metadata": {},
   "source": [
    "## Authenticate Client and Test Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764af861",
   "metadata": {},
   "source": [
    "We will now instantiate a Globus Compute client to test the function.  Globus will prompt the user for their credentials if running for the first time.  The user should have a Globus account through their ALCF account and should validate with their ALCF credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d63ca0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Globus Compute client\n",
    "# Import packages\n",
    "from globus_compute_sdk import Client, Executor\n",
    "import time\n",
    "gc = Client()\n",
    "polaris_endpoint_id = POLARIS_ENDPOINT_FOR_LLAMACPP_Llama38B\n",
    "gce = Executor(endpoint_id=polaris_endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67262fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = gce.submit(llamacpp,prompt=\"What are the proteins that interact with RAD51?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9fd693c4-4fcd-47a9-823c-b3ddfcc14699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '    \"id\": \"chatcmpl-65a0be86-53cc-458b-a3fb-bf16144c241c\",\\n'\n",
      " '    \"choices\": [\\n'\n",
      " '        {\\n'\n",
      " '            \"finish_reason\": \"stop\",\\n'\n",
      " '            \"index\": 0,\\n'\n",
      " '            \"logprobs\": null,\\n'\n",
      " '            \"message\": {\\n'\n",
      " '                \"content\": \"RAD51 is a key protein involved in homologous '\n",
      " 'recombination, a mechanism of DNA repair and genetic recombination. It '\n",
      " 'interacts with several other proteins to facilitate its function. Some of '\n",
      " 'the proteins that interact with RAD51 include:\\\\n\\\\n1. BRCA2: A tumor '\n",
      " 'suppressor protein that binds to RAD51 and helps to stabilize it at the site '\n",
      " 'of DNA damage.\\\\n2. PALB2: A protein that interacts with BRCA2 and RAD51, '\n",
      " 'facilitating their interaction and promoting homologous recombination.\\\\n3. '\n",
      " 'NBS1: A protein involved in the MRE11-RAD50-NBS1 complex, which is '\n",
      " 'responsible for detecting DNA double-strand breaks and recruiting RAD51 to '\n",
      " 'the damage site.\\\\n4. MRE11: A nuclease that interacts with RAD51 and helps '\n",
      " 'to process DNA ends during homologous recombination.\\\\n5. RAD54: A protein '\n",
      " 'that interacts with RAD51 and enhances its activity by promoting the '\n",
      " 'formation of a RAD51 filament on the DNA molecule.\\\\n6. DMC1: A protein '\n",
      " 'involved in meiotic recombination, which interacts with RAD51 and helps to '\n",
      " 'promote the exchange of genetic material between homologous '\n",
      " 'chromosomes.\\\\n7. XRCC3: A protein that interacts with RAD51 and helps to '\n",
      " 'stabilize it at the site of DNA damage.\\\\n8. XRCC2: A protein that interacts '\n",
      " 'with RAD51 and helps to facilitate its activity during homologous '\n",
      " 'recombination.\\\\n9. BRCA1: A tumor suppressor protein that interacts with '\n",
      " 'RAD51 and regulates its activity, particularly in response to DNA '\n",
      " 'damage.\\\\n10. FANCD2: A protein involved in the Fanconi anemia pathway, '\n",
      " 'which interacts with RAD51 and helps to regulate its activity.\\\\n\\\\nThese '\n",
      " 'proteins play important roles in regulating the activity of RAD51 and '\n",
      " 'facilitating homologous recombination during DNA repair and genetic '\n",
      " 'recombination.\",\\n'\n",
      " '                \"role\": \"assistant\"\\n'\n",
      " '            }\\n'\n",
      " '        }\\n'\n",
      " '    ],\\n'\n",
      " '    \"created\": 1716915781,\\n'\n",
      " '    \"model\": \"Meta-Llama-3-8B-Instruct-Q8_0.gguf\",\\n'\n",
      " '    \"object\": \"chat.completion\",\\n'\n",
      " '    \"usage\": {\\n'\n",
      " '        \"completion_tokens\": 373,\\n'\n",
      " '        \"prompt_tokens\": 34,\\n'\n",
      " '        \"total_tokens\": 407\\n'\n",
      " '    }\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb87e0-c1f7-43e6-ab7f-cf64759177f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32266c8d-303d-42f9-a448-0f2543ce9123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10-llama-cpp-env",
   "language": "python",
   "name": "llama-cpp-python-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
