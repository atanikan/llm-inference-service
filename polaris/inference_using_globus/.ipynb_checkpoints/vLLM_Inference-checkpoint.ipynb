{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3813a6",
   "metadata": {},
   "source": [
    "# vLLM Inference runs at ALCF from remote notebooks\n",
    "\n",
    "This notebook demonstrates how to run vLLM for serving LLM models on Polaris at ALCF using [Globus Compute](https://funcx.readthedocs.io/en/latest/endpoints.html).  In this example, we will authenticate using Globus Auth. Set up a compute endpoint on Polaris. Register a function that serves models using vLLM with Globus (FuncX) and subsequently launch that function in batch mode remotely so that it runs on Polaris and transfers results back to user.\n",
    "\n",
    "This notebook can be run from anywhere, it only requires a local installation of Globus software (described below) and access to a Globus Compute Endpoint setup by the user on Polaris that has access to vLLM (also described below).\n",
    "\n",
    "This demo uses Globus Compute (can also use Globus Flows if needed). Globus Compute is a remote executor for tasks expressed as python functions that are sent to remote machines following a fire-and-forget model.\n",
    "\n",
    "In this notebook we will first describe necessary setup tasks for the local environment and on Polaris; second, we will describe how to create and test a Globus Compute function that can remotely launch a vLLM on Polaris compute nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ac190-cb6b-494d-9e78-354c1830b518",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "1. Allocation on [Polaris](https://accounts.alcf.anl.gov/#/home)\n",
    "2. Access to [Globus](https://www.globus.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d4cc1",
   "metadata": {},
   "source": [
    "## Local Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d36389",
   "metadata": {},
   "source": [
    "This notebook can be run from anywhere.  The only requirement is a local environment, such as a conda environment or python, that has python 3.10 installed along with the Globus packages `globus_compute_sdk` and `globus_cli`.  For e.g.\n",
    "\n",
    "```bash\n",
    "python3.10 -m venv vllm-globus-env\n",
    "source activate vllm-globus-env/bin/activate\n",
    "pip install notebook globus_compute_sdk globus_cli\n",
    "python -m ipykernel install --user --name vllm-env --display-name \"Python3.10-vllm-env\"\n",
    "jupyter notebook\n",
    "```\n",
    "> **__Note:__** <br>\n",
    "> Change the kernel to point to the vllm env in your notebook. <br/>\n",
    "> The vllm environment on Polaris should also contain the same python version 3.10. It is therefore necessary for this environment on your local machine to have a python version close to this version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60401bc9",
   "metadata": {},
   "source": [
    "## Create a Globus Compute Endpoint on Polaris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f27c7",
   "metadata": {},
   "source": [
    "The first step for a user to execute applications on Polaris through the Globus service is to create a Globus compute endpoint on Polaris.  <b> This requires the user to do a one-time setup task to configure the endpoint </b>.\n",
    "\n",
    "In a shell seperate from this notebook, log into Polaris.  Copy the file included with this notebook called `vllm_template_config.yaml` and `requirements.txt` to the Polaris filesystem (doesn't matter where).  Inside `vllm_template_config.yaml` you should see options setting your <project name>, your queue (default `debug`), and commands that activate a conda environment on Polaris.\n",
    "\n",
    "In your shell on Polaris, execute the following commands:\n",
    "\n",
    "```bash\n",
    "module load conda\n",
    "conda create -p /eagle/<project_name>/env/vllm_env python==3.10 --y\n",
    "conda activate /eagle/<project_name>/env/vllm_env\n",
    "pip install -r requirements.txt\n",
    "globus-compute-endpoint configure --endpoint-config vllm_template_config.yaml vllm_endpoint\n",
    "globus-compute-endpoint start vllm_endpoint\n",
    "globus-compute-endpoint list\n",
    "```\n",
    "This will create an endpoint and display its status.  Its status should be listed as `running`.  There will also be displayed a unique Endpoint ID in the form of a UUID.  Copy that ID and paste it below as a string assigned to `POLARIS_ENDPOINT_FOR_VLLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decc84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARIS_ENDPOINT_FOR_VLLM = \"1debb802-53d2-4ccc-ad7c-378b101bcd6c\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac1fca",
   "metadata": {},
   "source": [
    "Your endpoint is now active as a daemon process running on the Polaris login node.  It is communicating with the Globus service and waiting for work.  If you ever want to stop the process you can run:\n",
    "```bash\n",
    "globus-compute-endpoint stop vllm_endpoint\n",
    "```\n",
    "Your process may need to be periodically restarted, for example after Polaris comes back from a maintance period.\n",
    "\n",
    "If you ever need to make changes to your endpoint configuration, you can find the settings file in `~/.globus_compute/vllm_endpoint/config.yaml`.  Edit this file and then restart the endpoint with `globus-compute-endpoint restart vllm_endpoint` to make the changes active.\n",
    "\n",
    "This endpoint will submit work to the `debug` queue or any other queue you have access to since this demo is for learning purposes.  In production, LLM users will be able to submit work to the [demand queue](https://docs.alcf.anl.gov/polaris/running-jobs/#queues) which will give immediate access to Polaris compute nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38640cb7",
   "metadata": {},
   "source": [
    "## Create a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d0caf",
   "metadata": {},
   "source": [
    "We first need to create a python function that wraps around the application call.  We will call it `inference_vllm_polaris`. Ensure you change the cache directories to a project folder your have access to within the function\n",
    "```bash\n",
    "os.environ['HF_DATASETS_CACHE'] = '/eagle/datascience/atanikanti/vllm/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/eagle/datascience/atanikanti/vllm/.cache'\n",
    "os.environ['HF_HOME'] = \"/eagle/datascience/atanikanti/vllm/.cache\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a08c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Globus Compute function\n",
    "def inference_vllm_polaris(\n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0.8,\n",
    "    model_name: str = 'meta-llama/Llama-2-70b-chat-hf',\n",
    "    tokenizer: str = 'hf-internal-testing/llama-tokenizer',\n",
    "    prompt: str = None,\n",
    "    tensor_parallel_size: int = 4\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Function to infer performance and generate outputs based on the parsed arguments on Polaris supercomputer.\n",
    "\n",
    "    Argument\n",
    "    --------\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        temperature (float): Sampling temperature\n",
    "        model_name (str): Name of the model\n",
    "        tokenizer (str): Name of the tokenizer\n",
    "        prompt (str): Prompt to generate\n",
    "        tensor_parallel_size (int): Size of the tensor parallel. No of GPUs used for inference\n",
    "        download_dir (str): Directory to download the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Import packages\n",
    "    import os\n",
    "    os.environ['HF_DATASETS_CACHE'] = '/eagle/datascience/atanikanti/vllm/.cache' #CHANGE THIS\n",
    "    os.environ['TRANSFORMERS_CACHE'] = '/eagle/datascience/atanikanti/vllm/.cache' #CHANGE THIS\n",
    "    os.environ['HF_HOME'] = \"/eagle/datascience/atanikanti/vllm/.cache\" #CHANGE THIS\n",
    "    from vllm import LLM, SamplingParams\n",
    "    import ray\n",
    "    ray.shutdown()\n",
    "    ray.init(_temp_dir='/tmp')\n",
    "    import time\n",
    "    import json\n",
    "\n",
    "    # Load image (PIL format)\n",
    "    print(f\"max_tokens: {max_tokens}, temperature: {temperature}, model_name: {model_name}, tokenizer: {tokenizer}, prompt: {prompt}, tensor_parallel_size: {tensor_parallel_size}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    params: SamplingParams = SamplingParams(max_tokens=max_tokens, temperature=temperature)\n",
    "    lm: LLM = LLM(\n",
    "        model=model_name, tokenizer=tokenizer, tensor_parallel_size=tensor_parallel_size\n",
    "    )\n",
    "    if not prompt:\n",
    "        prompts = [\n",
    "            \"Hello, my name is\",\n",
    "            \"The president of the United States is\",\n",
    "            \"The capital of France is\",\n",
    "            \"The future of AI is\",\n",
    "        ]\n",
    "        outputs = lm.generate(prompts, params)\n",
    "    else:\n",
    "        outputs = lm.generate([prompt], params)\n",
    "    stats_for_each = []\n",
    "    total_num_of_tokens = 0\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        num_tokens = len(output.outputs[0].token_ids)\n",
    "        total_num_of_tokens = total_num_of_tokens + num_tokens\n",
    "        stats_for_each.append({\n",
    "            \"num_tokens\": num_tokens,\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_text\": generated_text})\n",
    "    latency = time.time() - start_time\n",
    "    tokens_per_second = total_num_of_tokens / latency     \n",
    "    stats = {\n",
    "        \"latency\": f\"{latency:.2f} sec\",\n",
    "        \"tokens_per_second\": f\"{tokens_per_second:.2f} sec\",\n",
    "        \"stats\": stats_for_each\n",
    "    }\n",
    "    result = json.dumps(stats)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae3a4e",
   "metadata": {},
   "source": [
    "## Authenticate Client and Test Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764af861",
   "metadata": {},
   "source": [
    "We will now instantiate a Globus Compute client to test the function.  Globus will prompt the user for their credentials if running for the first time.  The user should have a Globus account through their ALCF account and should validate with their ALCF credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63ca0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Globus Compute client\n",
    "# Import packages\n",
    "from globus_compute_sdk import Client, Executor\n",
    "import time\n",
    "gc = Client()\n",
    "polaris_endpoint_id = POLARIS_ENDPOINT_FOR_VLLM\n",
    "gce = Executor(endpoint_id=polaris_endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67262fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TaskExecutionFailed",
     "evalue": "\n Traceback (most recent call last):\n   File \"/var/folders/f9/ff27tdm11x91185x3q1v6_lm0000gq/T/ipykernel_47813/1687978244.py\", line 60, in inference_vllm_polaris\n UnboundLocalError: local variable 'latency' referenced before assignment\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTaskExecutionFailed\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m future \u001b[38;5;241m=\u001b[39m gce\u001b[38;5;241m.\u001b[39msubmit(inference_vllm_polaris, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenlm-research/open_llama_13b\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_2/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTaskExecutionFailed\u001b[0m: \n Traceback (most recent call last):\n   File \"/var/folders/f9/ff27tdm11x91185x3q1v6_lm0000gq/T/ipykernel_47813/1687978244.py\", line 60, in inference_vllm_polaris\n UnboundLocalError: local variable 'latency' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "future = gce.submit(inference_vllm_polaris, temperature=0.9, model_name='openlm-research/open_llama_13b')\n",
    "print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b20d4",
   "metadata": {},
   "source": [
    "## Register Function (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcefda2",
   "metadata": {},
   "source": [
    "Now that the function has been tested and works, register the function with the Globus service.  This will allow the user to call the function from within a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742fd4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624aa05-0290-4028-b7da-c16b5a687fd2\n"
     ]
    }
   ],
   "source": [
    "reconstruction_func = gc.register_function(inference_vllm_polaris)\n",
    "\n",
    "print(inference_vllm_polaris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c34c26d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reconstructed data specified in inputOneSliceOfEach.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future = gce.submit_to_registered_function(args=[\"/eagle/IRIBeta/als/example\"], function_id=inference_vllm_polaris)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a4897-694b-40d8-a700-2bf9b793e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb87e0-c1f7-43e6-ab7f-cf64759177f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32266c8d-303d-42f9-a448-0f2543ce9123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 vllm env",
   "language": "python",
   "name": "vllm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
